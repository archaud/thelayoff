{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes classifiers\n",
    "\n",
    "Our goal is to \"train\" a NB classifier (really just count things) and apply it to some new documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple word definition, we could do more complicated things like identify latex segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_pattern = regex.compile(\"\\w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    docs = []\n",
    "    sum_counter = Counter()\n",
    "    total_tokens = 0\n",
    "    \n",
    "    with open(filename) as reader:\n",
    "        for line in reader:\n",
    "            fields = line.rstrip().split(\"\\t\")\n",
    "            if len(fields) < 3:\n",
    "                continue\n",
    "                \n",
    "            tokens = word_pattern.findall(fields[2])\n",
    "            counts = Counter(tokens)\n",
    "            sum_counter.update(tokens)\n",
    "            total_tokens += len(tokens)\n",
    "            \n",
    "            doc = { \"name\": fields[0], \"label\": fields[1], \"tokens\": tokens, \"counts\": counts }\n",
    "            docs.append(doc)\n",
    "    \n",
    "    return (sum_counter, total_tokens, docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'datascience10k.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-27143c1cad59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mds_counter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"datascience10k.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mstat_counter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstat_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstat_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"stats10k.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-a0bd147a1d46>\u001b[0m in \u001b[0;36mread_file\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtotal_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mfields\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'datascience10k.txt'"
     ]
    }
   ],
   "source": [
    "ds_counter, ds_tokens, ds_docs = read_file(\"datascience10k.txt\")\n",
    "stat_counter, stat_tokens, stat_docs = read_file(\"stats10k.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1260636 1636418\n"
     ]
    }
   ],
   "source": [
    "print(ds_tokens, stat_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 60439),\n",
       " ('to', 34888),\n",
       " ('a', 28704),\n",
       " ('of', 27683),\n",
       " ('I', 25196),\n",
       " ('is', 24455),\n",
       " ('and', 22722),\n",
       " ('in', 16448),\n",
       " ('you', 14814),\n",
       " ('that', 14015)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_counter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 84321),\n",
       " ('of', 39721),\n",
       " ('to', 37171),\n",
       " ('a', 35188),\n",
       " ('is', 33199),\n",
       " ('I', 28313),\n",
       " ('and', 28147),\n",
       " ('in', 20997),\n",
       " ('that', 19814),\n",
       " ('for', 16206)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat_counter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a union vocabulary so we have one list of all the words we know about\n",
    "\n",
    "everything_counter = ds_counter + stat_counter\n",
    "vocabulary = list(everything_counter.keys())\n",
    "vocab_size = len(vocabulary)\n",
    "vocab_set = set(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50059"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zipf's law\n",
    "\n",
    "Almost half the distinct terms occur exactly once, but about 100 words make up half of all the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 23095),\n",
       " (2, 6626),\n",
       " (3, 3387),\n",
       " (4, 2264),\n",
       " (5, 1565),\n",
       " (6, 1174),\n",
       " (7, 879),\n",
       " (8, 748),\n",
       " (9, 631),\n",
       " (10, 499)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there are 23k word types that occur once \n",
    "\n",
    "Counter(everything_counter.values()).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144760\t0.0500\t5.0%\tthe\n",
      "72059\t0.0249\t7.5%\tto\n",
      "67404\t0.0233\t9.8%\tof\n",
      "63892\t0.0221\t12.0%\ta\n",
      "57654\t0.0199\t14.0%\tis\n",
      "53509\t0.0185\t15.9%\tI\n",
      "50869\t0.0176\t17.6%\tand\n",
      "37445\t0.0129\t18.9%\tin\n",
      "33829\t0.0117\t20.1%\tthat\n",
      "29222\t0.0101\t21.1%\tfor\n",
      "29149\t0.0101\t22.1%\tyou\n",
      "22169\t0.0077\t22.8%\tit\n",
      "21416\t0.0074\t23.6%\tthis\n",
      "21251\t0.0073\t24.3%\t1\n",
      "21169\t0.0073\t25.1%\tare\n",
      "21055\t0.0073\t25.8%\tbe\n",
      "19968\t0.0069\t26.5%\twith\n",
      "18786\t0.0065\t27.1%\thave\n",
      "18145\t0.0063\t27.7%\tdata\n",
      "16914\t0.0058\t28.3%\ton\n",
      "16789\t0.0058\t28.9%\tas\n",
      "16514\t0.0057\t29.5%\tcan\n",
      "15049\t0.0052\t30.0%\tnot\n",
      "13553\t0.0047\t30.5%\tmodel\n",
      "13192\t0.0046\t30.9%\t2\n",
      "12758\t0.0044\t31.4%\t0\n",
      "12640\t0.0044\t31.8%\tor\n",
      "11936\t0.0041\t32.2%\tThe\n",
      "11788\t0.0041\t32.6%\tt\n",
      "11216\t0.0039\t33.0%\tyour\n",
      "10943\t0.0038\t33.4%\tbut\n",
      "10510\t0.0036\t33.7%\tfrom\n",
      "10504\t0.0036\t34.1%\tif\n",
      "10466\t0.0036\t34.5%\ts\n",
      "9936\t0.0034\t34.8%\twhich\n",
      "9782\t0.0034\t35.1%\tan\n",
      "9688\t0.0033\t35.5%\twould\n",
      "9272\t0.0032\t35.8%\tby\n",
      "8733\t0.0030\t36.1%\tuse\n",
      "8493\t0.0029\t36.4%\tone\n",
      "8120\t0.0028\t36.7%\twe\n",
      "8056\t0.0028\t37.0%\twill\n",
      "7898\t0.0027\t37.2%\tx\n",
      "7648\t0.0026\t37.5%\tthere\n",
      "7521\t0.0026\t37.8%\tdo\n",
      "7498\t0.0026\t38.0%\tusing\n",
      "7344\t0.0025\t38.3%\tsome\n",
      "7216\t0.0025\t38.5%\tmy\n",
      "7216\t0.0025\t38.8%\teach\n",
      "7038\t0.0024\t39.0%\tat\n",
      "6781\t0.0023\t39.2%\tlike\n",
      "6658\t0.0023\t39.5%\ti\n",
      "6508\t0.0022\t39.7%\tall\n",
      "6499\t0.0022\t39.9%\tso\n",
      "6477\t0.0022\t40.1%\tmore\n",
      "6383\t0.0022\t40.4%\thow\n",
      "6344\t0.0022\t40.6%\tthen\n",
      "6116\t0.0021\t40.8%\tam\n",
      "6082\t0.0021\t41.0%\twhat\n",
      "6064\t0.0021\t41.2%\tIf\n",
      "5934\t0.0020\t41.4%\tX\n",
      "5574\t0.0019\t41.6%\ttest\n",
      "5562\t0.0019\t41.8%\ttime\n",
      "5354\t0.0018\t42.0%\tIn\n",
      "5341\t0.0018\t42.2%\tvalues\n",
      "5326\t0.0018\t42.4%\tThis\n",
      "5301\t0.0018\t42.5%\ttwo\n",
      "5294\t0.0018\t42.7%\tn\n",
      "5258\t0.0018\t42.9%\tset\n",
      "5258\t0.0018\t43.1%\tm\n",
      "5142\t0.0018\t43.3%\tdifferent\n",
      "5120\t0.0018\t43.4%\tother\n",
      "5054\t0.0017\t43.6%\tshould\n",
      "5042\t0.0017\t43.8%\tabout\n",
      "5018\t0.0017\t44.0%\twant\n",
      "4946\t0.0017\t44.1%\tdistribution\n",
      "4905\t0.0017\t44.3%\texample\n",
      "4898\t0.0017\t44.5%\tvalue\n",
      "4891\t0.0017\t44.6%\thas\n",
      "4858\t0.0017\t44.8%\tfunction\n",
      "4848\t0.0017\t45.0%\te\n",
      "4828\t0.0017\t45.1%\twhere\n",
      "4749\t0.0016\t45.3%\tsame\n",
      "4739\t0.0016\t45.5%\tany\n",
      "4659\t0.0016\t45.6%\tfrac\n",
      "4594\t0.0016\t45.8%\tonly\n",
      "4561\t0.0016\t45.9%\t3\n",
      "4542\t0.0016\t46.1%\tbetween\n",
      "4515\t0.0016\t46.3%\tnumber\n",
      "4507\t0.0016\t46.4%\tproblem\n",
      "4478\t0.0015\t46.6%\tA\n",
      "4382\t0.0015\t46.7%\twhen\n",
      "4320\t0.0015\t46.9%\tcould\n",
      "4309\t0.0015\t47.0%\tthan\n",
      "4308\t0.0015\t47.2%\tYou\n",
      "4299\t0.0015\t47.3%\talso\n",
      "4267\t0.0015\t47.5%\tvariables\n",
      "4263\t0.0015\t47.6%\tget\n",
      "4172\t0.0014\t47.7%\tregression\n",
      "4169\t0.0014\t47.9%\tvariable\n"
     ]
    }
   ],
   "source": [
    "# 10 words make up about 20% of the collection\n",
    "\n",
    "cumulative_proportion = 0.0\n",
    "\n",
    "for w, c in everything_counter.most_common(100):\n",
    "    proportion = c / (ds_tokens + stat_tokens)\n",
    "    cumulative_proportion += proportion\n",
    "    \n",
    "    print(\"{}\\t{:.4f}\\t{:.1f}%\\t{}\".format(c, proportion, 100 * cumulative_proportion, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn counts into probability with Dirichlet smoothing\n",
    "\n",
    "def log_prob(word, counts, total, smoothing=0.1):\n",
    "    prob = (counts[word] + smoothing) / (total + vocab_size * smoothing)\n",
    "    return np.log( prob ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two example docs I'm pretty sure aren't in the training set\n",
    "\n",
    "test_stat_doc = \"Suppose X1 and X2 are independent. Both have Bernoulli distribution, equal to one with probability 0.5. What is the probability distribution of the product X1X2? What is the covariance between X1 and X1X2? I am a beginner at statistics and quite confused by these conceptions. Would you please offer some guidance? Thank you!\"\n",
    "test_ds_doc = \"Can you tell me cheap services to train ai models in the cloud, and that they are easy to configure. I would like something like Google Collab, simple but with more powerful gpu. I want the service to come with python installed so that I can program in the cloud directly and not install python on my pc.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_tokens(doc, smoothing=0.1):\n",
    "    total = 0\n",
    "    for token in word_pattern.findall(doc):\n",
    "        if token in vocab_set:\n",
    "            ds_log_prob = log_prob(token, ds_counter, ds_tokens, smoothing)\n",
    "            stat_log_prob = log_prob(token, stat_counter, stat_tokens, smoothing)\n",
    "            diff = ds_log_prob - stat_log_prob\n",
    "            total += diff\n",
    "            print(f\"{ds_log_prob:.02f}\\t{stat_log_prob:.02f}\\t{diff:.02f}\\t{token}\")\n",
    "        else:\n",
    "            print(\"skipping \", token)\n",
    "    print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuition for log probabilities\n",
    "\n",
    "It's helpful to have a rough idea what the scale of a log probability means. All log probabilities are negative or zero (why?) so if we drop the negative and exponentiate, we get the equivalent \"one in ...\" number.\n",
    "\n",
    "Note that in this class $\\log$ means natural log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.3025850929940455"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(1/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.000009070063655"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(2.302586)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log prob -1 ~ one in        2.7\n",
      "log prob -2 ~ one in        7.4\n",
      "log prob -3 ~ one in       20.1\n",
      "log prob -4 ~ one in       54.6\n",
      "log prob -5 ~ one in      148.4\n",
      "log prob -6 ~ one in      403.4\n",
      "log prob -7 ~ one in     1096.6\n",
      "log prob -8 ~ one in     2981.0\n",
      "log prob -9 ~ one in     8103.1\n",
      "log prob -10 ~ one in    22026.5\n",
      "log prob -11 ~ one in    59874.1\n",
      "log prob -12 ~ one in   162754.8\n",
      "log prob -13 ~ one in   442413.4\n",
      "log prob -14 ~ one in  1202604.3\n"
     ]
    }
   ],
   "source": [
    "for lp in range(1,15):\n",
    "    print(\"log prob -{} ~ one in {: 10.1f}\".format(lp, np.exp(lp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2465969639416065"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(-1.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "640"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_counter[\"Python\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat_counter[\"Python\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-7.22\t-6.47\t-0.75\tR\n",
      "-8.75\t-7.77\t-0.98\tr\n",
      "-7.59\t-9.41\t1.82\tPython\n",
      "-7.85\t-9.98\t2.12\tpython\n",
      "-5.56\t-5.45\t-0.11\tThe\n",
      "-3.04\t-2.97\t-0.07\tthe\n",
      "2.030811072432871\n"
     ]
    }
   ],
   "source": [
    "# Capitalization is consistently more stats-ey (negative)\n",
    "# (including \"the\" as a baseline)\n",
    "\n",
    "compare_tokens(\"R r Python python The the\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-9.31\t-8.55\t-0.75\tSuppose\n",
      "-11.05\t-10.01\t-1.04\tX1\n",
      "-4.35\t-4.33\t-0.02\tand\n",
      "-11.34\t-10.21\t-1.13\tX2\n",
      "-5.25\t-5.19\t-0.06\tare\n",
      "-8.84\t-7.41\t-1.44\tindependent\n",
      "-9.79\t-9.77\t-0.02\tBoth\n",
      "-5.29\t-5.38\t0.09\thave\n",
      "-11.12\t-9.89\t-1.23\tBernoulli\n",
      "-7.64\t-6.25\t-1.39\tdistribution\n",
      "-8.81\t-7.98\t-0.83\tequal\n",
      "-3.92\t-4.05\t0.13\tto\n",
      "-6.12\t-6.13\t0.00\tone\n",
      "-5.25\t-5.30\t0.05\twith\n",
      "-7.81\t-6.80\t-1.01\tprobability\n",
      "-5.93\t-5.58\t-0.35\t0\n",
      "-7.29\t-6.95\t-0.33\t5\n",
      "-7.19\t-7.26\t0.08\tWhat\n",
      "-4.28\t-4.16\t-0.11\tis\n",
      "-3.37\t-3.23\t-0.14\tthe\n",
      "-7.81\t-6.80\t-1.01\tprobability\n",
      "-7.64\t-6.25\t-1.39\tdistribution\n",
      "-4.15\t-3.99\t-0.17\tof\n",
      "-3.37\t-3.23\t-0.14\tthe\n",
      "-8.62\t-8.73\t0.11\tproduct\n",
      "-11.98\t-12.27\t0.29\tX1X2\n",
      "-7.19\t-7.26\t0.08\tWhat\n",
      "-4.28\t-4.16\t-0.11\tis\n",
      "-3.37\t-3.23\t-0.14\tthe\n",
      "-10.08\t-8.36\t-1.72\tcovariance\n",
      "-6.92\t-6.63\t-0.30\tbetween\n",
      "-11.05\t-10.01\t-1.04\tX1\n",
      "-4.35\t-4.33\t-0.02\tand\n",
      "-11.98\t-12.27\t0.29\tX1X2\n",
      "-4.25\t-4.32\t0.08\tI\n",
      "-6.38\t-6.52\t0.15\tam\n",
      "-4.12\t-4.11\t-0.01\ta\n",
      "-10.09\t-11.11\t1.02\tbeginner\n",
      "-6.43\t-6.23\t-0.20\tat\n",
      "-8.91\t-8.03\t-0.89\tstatistics\n",
      "-4.35\t-4.33\t-0.02\tand\n",
      "-8.42\t-8.32\t-0.10\tquite\n",
      "-9.25\t-9.29\t0.04\tconfused\n",
      "-6.14\t-5.96\t-0.18\tby\n",
      "-6.94\t-6.87\t-0.08\tthese\n",
      "skipping  conceptions\n",
      "-9.64\t-9.46\t-0.17\tWould\n",
      "-4.78\t-5.00\t0.23\tyou\n",
      "-8.63\t-9.09\t0.45\tplease\n",
      "-10.32\t-10.36\t0.03\toffer\n",
      "-6.20\t-6.33\t0.13\tsome\n",
      "-10.51\t-10.77\t0.26\tguidance\n",
      "-9.04\t-8.99\t-0.05\tThank\n",
      "-4.78\t-5.00\t0.23\tyou\n",
      "-13.875319392247103\n"
     ]
    }
   ],
   "source": [
    "compare_tokens(test_stat_doc, 10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1701.0483220209924"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(7.439)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-8.21\t-8.33\t0.12\tCan\n",
      "-4.78\t-5.00\t0.23\tyou\n",
      "-8.92\t-8.82\t-0.10\ttell\n",
      "-7.11\t-7.17\t0.06\tme\n",
      "-11.20\t-11.48\t0.28\tcheap\n",
      "-11.01\t-11.21\t0.19\tservices\n",
      "-3.92\t-4.05\t0.13\tto\n",
      "-7.00\t-8.77\t1.77\ttrain\n",
      "-10.51\t-12.18\t1.67\tai\n",
      "-7.14\t-7.18\t0.04\tmodels\n",
      "-4.67\t-4.62\t-0.05\tin\n",
      "-3.37\t-3.23\t-0.14\tthe\n",
      "-10.13\t-11.40\t1.26\tcloud\n",
      "-4.35\t-4.33\t-0.02\tand\n",
      "-4.83\t-4.68\t-0.15\tthat\n",
      "-6.89\t-6.85\t-0.04\tthey\n",
      "-5.25\t-5.19\t-0.06\tare\n",
      "-8.65\t-8.83\t0.18\teasy\n",
      "-3.92\t-4.05\t0.13\tto\n",
      "-11.49\t-11.87\t0.38\tconfigure\n",
      "-4.25\t-4.32\t0.08\tI\n",
      "-6.04\t-5.96\t-0.07\twould\n",
      "-6.18\t-6.52\t0.34\tlike\n",
      "-7.55\t-7.66\t0.11\tsomething\n",
      "-6.18\t-6.52\t0.34\tlike\n",
      "-9.06\t-10.42\t1.35\tGoogle\n",
      "-11.98\t-12.27\t0.29\tCollab\n",
      "-7.72\t-7.77\t0.05\tsimple\n",
      "-5.86\t-5.88\t0.02\tbut\n",
      "-5.25\t-5.30\t0.05\twith\n",
      "-6.33\t-6.46\t0.13\tmore\n",
      "-10.01\t-10.37\t0.36\tpowerful\n",
      "-11.09\t-12.27\t1.19\tgpu\n",
      "-4.25\t-4.32\t0.08\tI\n",
      "-6.55\t-6.75\t0.20\twant\n",
      "-3.37\t-3.23\t-0.14\tthe\n",
      "-10.62\t-10.86\t0.24\tservice\n",
      "-3.92\t-4.05\t0.13\tto\n",
      "-8.77\t-8.78\t0.01\tcome\n",
      "-5.25\t-5.30\t0.05\twith\n",
      "-8.16\t-10.12\t1.96\tpython\n",
      "-10.43\t-11.48\t1.05\tinstalled\n",
      "-6.40\t-6.39\t-0.02\tso\n",
      "-4.83\t-4.68\t-0.15\tthat\n",
      "-4.25\t-4.32\t0.08\tI\n",
      "-5.37\t-5.55\t0.18\tcan\n",
      "-9.47\t-9.65\t0.19\tprogram\n",
      "-4.67\t-4.62\t-0.05\tin\n",
      "-3.37\t-3.23\t-0.14\tthe\n",
      "-10.13\t-11.40\t1.26\tcloud\n",
      "-8.85\t-8.92\t0.07\tdirectly\n",
      "-4.35\t-4.33\t-0.02\tand\n",
      "-5.63\t-5.50\t-0.13\tnot\n",
      "-10.13\t-11.17\t1.04\tinstall\n",
      "-8.16\t-10.12\t1.96\tpython\n",
      "-5.36\t-5.51\t0.15\ton\n",
      "-6.20\t-6.37\t0.17\tmy\n",
      "-11.74\t-12.18\t0.43\tpc\n",
      "18.703123887004303\n"
     ]
    }
   ],
   "source": [
    "compare_tokens(test_ds_doc, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation as a classifier\n",
    "\n",
    "NB isn't the best classifier, but it's pretty close, and the fact that it's just counting things makes leave-one-out cross validation really easy. We don't want to test on the training set, but we can easily create a test set of one for each doc by pretending we haven't seen it.\n",
    "\n",
    "Note that I'm leaving out the prior probability of data science vs. stats, since I made this 50/50 by construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1260636"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DS\t8882\t9913\n"
     ]
    }
   ],
   "source": [
    "smoothing = 1.0\n",
    "\n",
    "num_correct = 0\n",
    "num_docs = 0\n",
    "\n",
    "for doc in ds_docs:\n",
    "    # pretend we haven't seen this before\n",
    "    for word, count in doc[\"counts\"].items():\n",
    "        ds_counter[word] -= count\n",
    "        ds_tokens -= count\n",
    "    \n",
    "    total_diff = 0.0\n",
    "    for token in doc[\"tokens\"]:\n",
    "        if token in vocab_set:\n",
    "            ds_log_prob = log_prob(token, ds_counter, ds_tokens, smoothing)\n",
    "            stat_log_prob = log_prob(token, stat_counter, stat_tokens, smoothing)\n",
    "            diff = ds_log_prob - stat_log_prob\n",
    "            total_diff += diff\n",
    "    \n",
    "    if total_diff > 0.0:\n",
    "        num_correct += 1\n",
    "    num_docs += 1\n",
    "    \n",
    "    # add it back in\n",
    "    for word, count in doc[\"counts\"].items():\n",
    "        ds_counter[word] += count\n",
    "        ds_tokens += count\n",
    "    \n",
    "    if ds_tokens != 1260636:\n",
    "        print(doc)\n",
    "        break\n",
    "\n",
    "print(f\"DS\\t{num_correct}\\t{num_docs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1260636"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stat\t7421\t9937\n"
     ]
    }
   ],
   "source": [
    "smoothing = 1.0\n",
    "\n",
    "num_correct = 0\n",
    "num_docs = 0\n",
    "\n",
    "for doc in stat_docs:\n",
    "    # pretend we haven't seen this before\n",
    "    for word, count in doc[\"counts\"].items():\n",
    "        stat_counter[word] -= count\n",
    "        stat_tokens -= count\n",
    "    \n",
    "    total = 0.0\n",
    "    for token in doc[\"tokens\"]:\n",
    "        if token in vocab_set:\n",
    "            ds_log_prob = log_prob(token, ds_counter, ds_tokens, smoothing)\n",
    "            stat_log_prob = log_prob(token, stat_counter, stat_tokens, smoothing)\n",
    "            diff = ds_log_prob - stat_log_prob\n",
    "            total += diff\n",
    "    \n",
    "    if total < 0.0:\n",
    "        num_correct += 1\n",
    "    num_docs += 1\n",
    "    \n",
    "    # add it back in\n",
    "    for word, count in doc[\"counts\"].items():\n",
    "        stat_counter[word] += count\n",
    "        stat_tokens += count\n",
    "\n",
    "print(f\"stat\\t{num_correct}\\t{num_docs}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
